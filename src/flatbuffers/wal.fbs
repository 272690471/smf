// Copyright 2017 Alexander Gallego
//
namespace smf.wal;

// TAG: alpha

enum wal_entry_compression_type:byte {
  none,
  zstd,
  lz4
}

/// WARNING
///
/// \brief on the actual disk format, we manually write one int at a time
/// via seastar::write_le and seastar::read_le
/// for example: seastar::write_le<uint8_t>(compression)
/// this saves about 19 bytes per entry on disk! and we still have a bunch
/// of zeros for lower sizes.
///
/// On the reader we allocate 9 bytes for reading, read one field at a time
/// and populate the struct manually afterwards.
///
/// We write these fields in ORDER:
///     size(4 bytes), checksum(4 bytes), compression(1 byte).
///
struct wal_header {
  size:        uint;
  checksum:    uint;
  compression: wal_entry_compression_type;
}


enum tx_put_operation:byte {
  none,
  begin,
  data,
  commit,
  abort,

  /// \brief either the end of the txn OR a full transaction that is a small payload
  /// i.e.: fits in one request
  full
}

enum tx_put_invalidation_reason:byte {
  none,
  server_connection_timeout,
  client_connection_timeout
}

enum tx_put_fragment_type:byte {
  none,
  kv,
  invalidation
}

/// \brief This is the datastructure that gets persisted on disk.
/// Please be tender while extending
/// When you extend this, please don't forget to change
/// \code
///    filesystem/wal_write_projection.cc::xform() method
/// \endcode
///
table tx_put_fragment {
  /// what should we do with this trasaction fragment
  op:           tx_put_operation = none;
  /// holds what kind of data we expect in the payload
  type:         tx_put_fragment_type = none;
  /// In seastar clients we will use a losely ticked timer which moves
  /// every 10 micros.
  /// This is needed for systems doing window aggregations, etc.
  /// expected resolution is milliseconds
  epoch_ms:     ulong = 0;

  /// The reason key=value *and* invalidations are part of the same
  /// struct is performance. every table is 12 bytes

  key:              [ubyte];
  value:            [ubyte];

  invalidation_reason: tx_put_invalidation_reason = none;
  /// iff this->op == tx_put_operation::abort
  /// use to write an abort, for a specific offset
  /// This is the offset from the tx_put_reply.
  invalidation_offset: ulong = 0;

}

table tx_put_partition_tuple {
  /// hashing_utils::xxhash_32( topic +  tx_put_fragment::data::key )
  partition:   uint (key); 

  // 
  /// \brief - this could be optimized further if we make the client send bytes instead.
  /// currently kafka and other brokers do many re-compression
  ///   1) form the transport RPC mechanism
  ///   2) from the disk format and ultimate disk layout
  /// There is also at least one memcpy of all bytes into the caches:
  ///   1) To the disk cache - we keep a 1MB write-behind cache if the direct io
  ///      is catching behind
  ///   2) To the internal - page cache - since we don't use the operating system
  ///      which must keep 1MB per open file.
  ///
  /// We have locks, and many other mem copies the java.nio.channels.FileChannel
  /// uses for example https://goo.gl/hrTnjY
  /// since we don't then again copy into the page cache.
  ///
  /// Last, we don't keep any logs. Currently the kafka path is synchronized 3 times.
  /// 1) To get the leader & append the writes for the log section.
  /// 1.1) Roll the file - which happens under a relatively big critical region inside Log.scala
  /// 2) Into the operating system page cache
  /// 3) Into the java nio channel FileChannel since it 'guarantees' concurrent friendly updates
  ///
  txs:         [tx_put_fragment];
}

/// brief - stores `puts` transactionally
table tx_put_request {
  topic:   string;
  data:    [tx_put_partition_tuple];
}

table tx_put_reply_partition_tuple {
  topic:       string;
  partition:   uint (key);
  /// \brief the committed offset into the WAL
  ///  clients can monitor how much disk was consumed by the puts
  ///  and we need this for cache eviction and management.
  start_offset: ulong;
  end_offset:   ulong;

}
table tx_put_reply {
  offsets: [tx_put_reply_partition_tuple];
}


table tx_get_request {
  topic:      string;
  partition:  uint;
  offset:     ulong;
  /// (1 << 31) - 1 Max payload payload by flatbuffers 2GB-1
  /// we decrease it by 100 bytes so we can stuff headers in there
  /// plus plenty of room for growth
  ///
  /// >>> (2**31)-100
  /// 2147483548
  ///
  /// default is 2MB -> 2048 * 1024
  max_bytes:  uint = 2097152;
}

/// \brief the broker might have decided to compress the tx_put_fragment.
/// 
table tx_get_fragment {
  hdr:             wal_header;
  /// \brief nothing more than a byte array of a possibly compressed
  /// tx_put_fragment
  fragment:        [ubyte]; 
}
enum wal_read_errno:byte {
  /// \brief - we chillin
  none,

  /// \brief the offset does not exist
  invalid_offset,

  /// \brief header for WAL entry is corrupted
  invalid_header,

  /// \brief invalid checksum for the payload
  missmatching_payload_checksum,

  /// \brief the header of the entry says the payload is of size
  /// and that size does not match with the actual data read
  /// from the disk
  missmatching_header_payload_size,
}
table tx_get_reply {
  next_offset: ulong;
  gets:        [tx_get_fragment];
  error:       wal_read_errno = none;
}

table wal_watermark {
  topic:          string;
  partition:      uint;
  low_watermark:  ulong;
  high_watermark: ulong;
}